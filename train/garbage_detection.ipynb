{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection with Keras and TS\n",
    "\n",
    "- Documentation : https://www.tensorflow.org/hub/tutorials/object_detection?hl=fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tflite-model-maker\n",
    "%pip install -q tflite-support\n",
    "%pip install opencv-python-headless==4.5.2.52\n",
    "!pip3 install pycocotools\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "# pylint: disable=g-import-not-at-top\n",
    "  from pycocotools.coco import COCO\n",
    "  from pycocotools.cocoeval import COCOeval\n",
    "# pylint: enable=g-import-not-at-top\n",
    "except ImportError:\n",
    "  print('Erreur')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'datasets/dataset.voc/train'\n",
    "val_folder = 'datasets/dataset.voc/valid'\n",
    "test_folder = 'datasets/dataset.voc/test'\n",
    "\n",
    "# These images are used to train the object detection model to recognize plastic and can\n",
    "train_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    train_folder,\n",
    "    train_folder,\n",
    "    ['plastique','canette']\n",
    ")\n",
    "\n",
    "# These images are used to evaluate the model. \n",
    "# These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid overfitting.\n",
    "val_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    val_folder,\n",
    "    val_folder,\n",
    "    # label_map={1: \"plastique\", 2: \"canette\"}\n",
    "    ['plastique','canette']\n",
    ")\n",
    "\n",
    "# These images are used to evaluate the final model performance.\n",
    "test_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    test_folder,\n",
    "    test_folder,\n",
    "    ['plastique','canette']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en cache des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_data = train_data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles possibles : https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/model_spec\n",
    "# spec = model_spec.get('mobilenet_v2')\n",
    "# spec = model_spec.get('efficientdet_lite4')\n",
    "\n",
    "verbose=1\n",
    "tflite_max_detections=2\n",
    "\n",
    "spec = model_spec.get('efficientdet_lite3')\n",
    "spec.tflite_max_detections=tflite_max_detections\n",
    "spec.verbose=verbose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner entièrement le modèle (fine-tuning) ?\n",
    "train_whole_model=True\n",
    "epochs=80\n",
    "batch_size=5\n",
    "model = object_detector.create(train_data, \n",
    "                               validation_data=val_data,\n",
    "                               model_spec=spec, \n",
    "                               train_whole_model=train_whole_model,\n",
    "                               epochs=epochs,\n",
    "                               batch_size=batch_size\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.summary()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cocodataset.org/#detection-eval\n",
    "# val_loss, val_accuracy = model.evaluate(val_data)\n",
    "\n",
    "evaluation = model.evaluate(val_data)\n",
    "\n",
    "def plotEvaluation(eval, title=\"Résultats de l\\'évaluation du modèle'\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.bar(eval.keys(), eval.values(), color='skyblue')\n",
    "    plt.xlabel('Métrique')\n",
    "    plt.ylabel('Valeur')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotEvaluation(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export du modèle au format Tensorflow lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = QuantizationConfig.for_float16()\n",
    "model.export(export_dir='bin/', \n",
    "             tflite_filename='greengardians.tflite', \n",
    "            #quantization_config=config\n",
    "             )\n",
    "# model.export(export_dir='bin/', \n",
    "#              quantization_config=config,\n",
    "#              export_format=[ExportFormat.SAVED_MODEL, ExportFormat.LABEL])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'bin/greengardians.tflite'\n",
    "evaluation_lite = model.evaluate_tflite(model_path, test_data)\n",
    "\n",
    "plotEvaluation(evaluation_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_TPUS =  2\n",
    "\n",
    "# !edgetpu_compiler 'bin/greengardians.tflite' --num_segments=$NUMBER_OF_TPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load the labels into a list\n",
    "classes = ['???'] * model.model_spec.config.num_classes\n",
    "label_map = model.model_spec.config.label_map\n",
    "for label_id, label_name in label_map.as_dict().items():\n",
    "  classes[label_id-1] = label_name\n",
    "\n",
    "# classes = {0: \"plastique\", 1: \"canette\"}\n",
    "print('classes', classes)\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "  return resized_img, original_image\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "\n",
    "  signature_fn = interpreter.get_signature_runner()\n",
    "\n",
    "  # Feed the input image to the model\n",
    "  output = signature_fn(images=image)\n",
    "\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  count = int(np.squeeze(output['output_0']))\n",
    "  scores = np.squeeze(output['output_1'])\n",
    "  classes = np.squeeze(output['output_2'])\n",
    "  boxes = np.squeeze(output['output_3'])\n",
    "\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path,\n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "  print('results', results)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute\n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * original_image_np.shape[1])\n",
    "    xmax = int(xmax * original_image_np.shape[1])\n",
    "    ymin = int(ymin * original_image_np.shape[0])\n",
    "    ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [int(c) for c in COLORS[class_id]]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "model_path = 'bin/greengardians.tflite'\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# # Run inference and draw detection result on the local copy of the original file\n",
    "DETECTION_THRESHOLD = 0.5\n",
    "          \n",
    "PROJECT_FOLDER = Path(Path.cwd())\n",
    "image_folder = str(PROJECT_FOLDER) + '/datasets/dataset.voc/test/'\n",
    "# imageToPredict = 'datasets/dataset.voc/test/IMG_20231025_095509_jpg.rf.c83cb3628dda974ef28a37ce404b9ab7.jpg'\n",
    "                                                          \n",
    "l_detection_result_image = []\n",
    "files = [image_folder +'/'+ f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "\n",
    "# imageToPredict = 'datasets/dataset.voc/test/IMG_20231025_095509_jpg.rf.c83cb3628dda974ef28a37ce404b9ab7.jpg'\n",
    "imageToPredict = random.choice(files)\n",
    "print(imageToPredict)\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "        imageToPredict,\n",
    "        interpreter,\n",
    "        threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "# for file in files:\n",
    "#     detection_result_image = run_odt_and_draw_results(\n",
    "#         file,\n",
    "#         interpreter,\n",
    "#         threshold=DETECTION_THRESHOLD\n",
    "#     )\n",
    "#     l_detection_result_image.append(detection_result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(imageToPredict)\n",
    "plt.show()\n",
    "plt.close()\n",
    "# test_validation_folder = str(PROJECT_FOLDER) + '/datasets/dataset.voc/test_val/'\n",
    "\n",
    "# # Show all the detection result\n",
    "# for image in l_detection_result_image:\n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "#     # Save the image in the validation folder    \n",
    "#     plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
